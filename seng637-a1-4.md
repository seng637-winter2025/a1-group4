>   **SENG 637 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: Group 4     |
|-----------------|
| Pahul Brar                |   
|  Matheus De Morais Leça            |   
| Samin Hazeri              |   
| Saba Soghraty               |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Test Logistics](#_Toc439194679)

[4 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[5 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[6 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[7 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[8 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

This report summarizes our work testing the proposed system for the first assignment of SENG 637. The system under test for this lab was an ATM simulation system capable of performing the basic operations of a real ATM, such as withdrawals, deposits, and account inquiries.

Before this lab, our team had some familiarity with exploratory and manual functional testing. Finding and reporting bugs without following a script or being guided by use cases is quite common in a software development environment, especially when performing Quality Assurance duties. Even as hobbyists and users of open-source software, we often report bugs through a bug tracker to help developers improve their projects, hoping that these issues will be addressed in future releases.

Typically, our bug-reporting process involves testing all successful scenarios, testing all failure scenarios, and examining all possible edge and boundary conditions. This comprehensive approach helps ensure that we cover all features and expected behaviors of the software under investigation.

# High-level description of the exploratory testing plan

### **Test Plan Overview**

### **Key Details:**

* **Target Functions:**  
  * ATM card validation and PIN entry  
  * Cash withdrawal  
  * Deposit  
  * Fund transfer between accounts  
  * Balance inquiry  
  * Transaction cancellation  
  * Error handling (invalid PIN, transaction failure)  
  * Receipt generation  
  * Log management  
  * Key-operated switch behavior (operator start/stop)  
* **Test Approach:**  
  * **Balanced Testing Approach:** Given the number of functions, We’ll target testing a few functions in depth while also testing some critical paths that touch multiple components (e.g., successful and unsuccessful transactions, logging, and communication with the bank).  
  * Test both **common paths** (e.g., withdrawal of cash, balance inquiry) and **exceptional paths** (e.g., invalid PIN, canceling a transaction, system error scenarios).  
  * We’ll focus on testing the interactions with the bank system and ensure the communication, approvals, and error messages work as expected.  
  * We’ll also test edge cases where the ATM might behave unexpectedly, such as multiple rapid requests, incorrect card insertion, or transaction failure scenarios.  
  * **Sessions will be iterative**: Testing for different valid and invalid scenarios will be done sequentially while recording any defects or discrepancies.  
* **Test Case Generation:**  
  * Start by using **happy path scenarios**:  
    1. **Card Insertion & PIN Validation**: Test valid card and PIN, invalid PIN up to the third attempt, and permanent retention of the card.  
    2. **Cash Withdrawal**: Ensure the system accepts multiple withdrawal requests and handles insufficient funds.  
    3. **Deposit**: Test deposits.  
    4. **Balance Inquiry**: Test multiple accounts linked to the same card.  
  * Then test **error handling scenarios**:  
    1. **Cancel Transaction**: Press the cancel button in the middle of various transactions.  
    2. **Transaction Rejection**: Test scenarios where the bank rejects a transaction due to insufficient funds or other issues.  
    3. **Invalid Card**: Test system behavior with an invalid card or a card with expired data.  
    4. **Machine Shutdown**: Verify system response to operator shutdown during active customer interaction.  
    5. **Envelope Timeout**: Test the timeout period for deposits when an envelope is not inserted.  
  * Explore **boundary conditions** ( withdrawal of the maximum allowed amount, maximum number of attempts for PIN, minimum and maximum cash deposit).

### **Testing Scope**

1. **Card and PIN Interaction**:  
   * Verify that the ATM handles valid and invalid cards/PINs.  
   * Validate that after three incorrect PIN attempts, the card is permanently retained.  
2. **Transactions**:  
   * **Cash Withdrawal**: Ensure the ATM dispenses cash in multiples of $20. Test successful transactions, failed transactions (e.g., insufficient funds), and cancelation during withdrawal.  
   * **Deposit**: Verify envelope deposit (cash/check) handling. Check for timeout scenarios, successful deposits, and error handling.  
   * **Fund Transfer**: Ensure fund transfer between two accounts is processed and logged correctly.  
   * **Balance Inquiry**: Confirm correct balance information is displayed for different accounts.  
3. **Cancel Transaction**:  
   * Verify that pressing the Cancel key stops the current transaction and returns to the main screen, without executing any action.  
4. **Receipt Generation**:  
   * Verify that after each successful transaction (withdrawal, deposit, transfer, balance inquiry), a receipt is printed with the correct details (e.g., date, location, amount, account balances).  
5. **Error Handling**:  
   * Verify that when a transaction fails, the ATM displays the correct error message and prompts the user for further action (e.g., do they want to try again or cancel).  
   * Test how the ATM reacts to system failures or interruptions (e.g., network issues with the bank system).  
6. **Operator Controls**:  
   * Verify the behavior when the ATM is started/stopped using the key-operated switch. Test the operator’s ability to enter cash on hand and manage system shutdown during customer service.  
7. **Transaction Logs**:  
   * Verify that each transaction is logged correctly. Check that sensitive information is not logged, and that the system accurately logs both successful and failed transactions.

### **Expected Outcome**

The ATM should behave as described in the requirements, with:

* Proper handling of user inputs (PIN, transaction amounts, etc.).  
* Accurate transactions (withdrawals, deposits, transfers).  
* Correct error messages when transactions fail.  
* Timely transaction log updates.  
* Reliable operator interaction through the key switch and log management.

# Test Logistics

## Who Conducted the Test?

A team of four members was designated to conduct the testing phase. For the Exploratory testing we came up with the above plan, and tested it all out individually, this way we could each approach the task in our own way, and ensure we can find bugs that might be missed following a strict plan.

## How Long Did the Test Last?

The test lasted for 5 days. On the first day, all members conducted exploratory testing and took notes on any bugs discovered. Bugs reported during this phase would be reexamined during regression testing. Over the next two days, manual scripted testing was performed following a designated test suite based on the use cases. The test cases were split among the members, with each individual responsible for testing specific features and verifying whether the outcomes matched the expected results. Any unexpected behavior observed during both the exploratory and manual scripted testing phases was documented in the bug tracker as an issue.

During the final two days, the team checked whether the bugs had been fixed in the new system version. If a bug was fixed, the corresponding issue in the bug tracker was updated to "Done" and closed. Otherwise, the issue was updated to "In Progress" to indicate that the bug persisted in the most recent version of the system.

## Who Tested Each Functionality?

Below is a list of each member and their assigned test cases:

| Member   | Test Case #   |
|----------|-------------|
| Pahul Brar | 1-10 |
| Samin Hazeri | 11-20  |
| Saba Soghraty  | 21-30  |
| Matheus De Morais Leça | 31-40  |

# Comparison of exploratory and manual functional testing

Exploratory testing invovles a lot of brute force, involving following the expected requirements and attempting to find bugs by experimentation. While it does help in finding bugs, its not the most ideal. It innovles a lot of just playing around with the application. In our case, we made a test plan using the provided descrption of the ATM and then we are able to find 4 bugs using it. This was because our exploratory test plan focused on folowing two scenarios, a happy path scenario which tracked postive results, and a scenario looking at edge cases. This was a lot of work, as we were just randomly testing things. A benefit of EFT is that since everyone play around with the application in their own different unpredicatable way, we are ablt to find bugs that one might not otherwise. For example, when we were doing the exploratory testing, I wanted to see if after entering the wrong pin 3 times, if I could still re insert the same card, and to my suprise I could. Which makes no sense, as if the card has been witheld, how can I inser it again, and even if I can, it should have been locked. This was an interesting bug, which I only found because I wanted to test something many would not even think about. While its tradeoff is time, and energy, it does have its benefits.

In Manual Functional Testing, we were able to take things further, as now we knew specefic functions to target, and what the expected results should have been. This allowed for a much smmoother process, and we were able to capture 7 more bugs, this was surprising. We found that following a set plan was much easier, and allowed us to easily find bugs. Since the steps were laid out, it allowed us to quickly go through a test and find if there were any bugs. The tradeoff with MFT is, that the test suite is rather limited, as its built to a limited scope of functioanlites, and might miss out on finding bugs that are unexpected. Regardle, MFT is the most efficent way of testing an application, as it ensures that there is a well laid out plan. 

Overall, we found more bugs using the Manual Functional Testing Process compared to the Exploratory Testing, mainly due to having set guidelines, that ensure everyone is on the same page. The Functional testing ensured that all the basic functions were tested, even if not every edge scenario was. This is crucial in any application, as we want the user to be able to do the functional items without issue.


# Notes and discussion of the peer reviews of defect reports

For our defect reports we aimed to ensure we were as descriptive as possible, by following the suggested guide for each bug. In terms of rating, we tried to use the rating system to divide bugs from Lowest Priority to Highest Prioty, mainly focused on what are 100% neccessary for a customer compared to what a customer might want. For example while not every customer wants to transfer money from their Savings to their Money Market Account (Given Medium Priority), every customer wants to check the balance of their savings acount (Given High Priority). We tried following this logic in our bug reports to ensure that we were able to order the bugs in an appropriate way.

We found that the process was effective and well recieved by everyone, and ensure our bug reporting was at a high level.

# How the pair testing was managed and team work/effort was divided 

For the pair testing we split into teams of two, to tackle the problem, and combined our work in the end. The teams were Pahul and Matheus and Samin and Saba.  In terms of work, we divided the 40 test cases amongst ourselves, and worked on 10 each, ensuring we could do a quaility job. We wanted everyone to be able to have some experience with finding bugs, as well as reporting them. We found this system worked best for us, and we were pretty effecting in our roles. 

# Difficulties encountered, challenges overcome, and lessons learned

Testing the system was relatively straightforward, thanks to the clear instructions provided for performing its operations. However, we encountered some difficulties with the test suite. Several test cases were ambiguous, forcing us to decipher the intended expected outcomes and inputs. Despite these challenges, the testing process ran smoothly, and we successfully covered all the system's features.

We also faced some diffculties with the Jira Board setup, as we had only free accounts. We had to navigate and create a site, and the add each other as site adminstrators to be able to access the Kanban board, this process was a rather difficult apporach and made it very difficult to start.

# Comments/feedback on the lab and lab document itself

This lab played an important role in enhancing our collaborative skills, demonstrating the benefits of teamwork over working alone. It also underscored the significance of maintaining a robust bug-tracking system and ensuring that all issues are thoroughly resolved in updated versions of the system.
